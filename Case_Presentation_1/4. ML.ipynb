{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e11d226f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "21392de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained Word2Vec model.\n",
    "model = gensim.models.Word2Vec.load(\"w2v_model/word2vec_WordNumBig2.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d6028bb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_size = model.wv.vector_size\n",
    "vector_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c461bebb",
   "metadata": {},
   "source": [
    "# 驗證集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d9928e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = pd.read_pickle(\"clean_Validation.pk\")\n",
    "text2vec = []\n",
    "for text in val_df.clean_words:\n",
    "    wordNumber = 0\n",
    "    totalWordVec = np.zeros(vector_size)\n",
    "    for word in text:\n",
    "        try:\n",
    "            totalWordVec += model.wv[word]\n",
    "            wordNumber += 1\n",
    "        except:\n",
    "            continue\n",
    "    text2vec.append(totalWordVec/wordNumber)\n",
    "x_val = np.array(text2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f3ef4f",
   "metadata": {},
   "source": [
    "# 訓練集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "41c36c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "keyWords={\"obesity\":4,\n",
    "          \"obese\":4,\n",
    "          \"morbidly\":2,\n",
    "          \"morbid\":2,\n",
    "          \"hyperlipidemia\":2,\n",
    "          \"asthma\":1,\n",
    "          \"htn\":1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0f602364",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_pickle(\"clean_Test_Intuitive_VAL.pk\")\n",
    "train_df = pd.read_pickle(\"clean_Train_Textual.pk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3eaeba48",
   "metadata": {},
   "outputs": [],
   "source": [
    "text2vec = []\n",
    "for text in test_df.clean_words:\n",
    "    wordNumber = 0\n",
    "    totalWordVec = np.zeros(vector_size)\n",
    "    for word in text:\n",
    "        try:\n",
    "            if word in keyWords:\n",
    "                totalWordVec += keyWords[word] * model.wv[word]    # 加權\n",
    "            else:\n",
    "                totalWordVec += 0.1 * model.wv[word]\n",
    "            wordNumber += 1\n",
    "        except:\n",
    "            continue\n",
    "    text2vec.append(totalWordVec/wordNumber)\n",
    "\n",
    "# 多train\n",
    "# for text in train_df.clean_words:\n",
    "#     wordNumber = 0\n",
    "#     totalWordVec = np.zeros(vector_size)\n",
    "#     for word in text:\n",
    "#         try:\n",
    "#             if word in keyWords:\n",
    "#                 totalWordVec += keyWords[word] * model.wv[word]    # 加權\n",
    "#             else:\n",
    "#                 totalWordVec += 0.1 * model.wv[word]\n",
    "#             wordNumber += 1\n",
    "#         except:\n",
    "#             continue\n",
    "#     text2vec.append(totalWordVec/wordNumber)    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "X = np.array(text2vec)\n",
    "y = test_df.y\n",
    "# y = np.concatenate((test_df.y , train_df.y), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8bc61e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7c5c245e",
   "metadata": {},
   "source": [
    "# 分割資料成 X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f469c7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287518ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8389226d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2884bdef",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "49f293d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion_matrix: \n",
      " [[30  3]\n",
      " [ 9 38]]\n",
      "precison: 0.926829268292683\n",
      "recall: 0.8085106382978723\n",
      "accuracy: 0.85\n",
      "f1: 0.8636363636363636\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(max_depth=300, n_estimators=400)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# test: 計算confusion_matrix和相關驗證數值\n",
    "pred = clf.predict(X_test)\n",
    "c = confusion_matrix(y_test, pred)\n",
    "print(\"confusion_matrix: \\n\", c)\n",
    "accuracy = accuracy_score(y_test, pred)\n",
    "precision = precision_score(y_test, pred)\n",
    "recall = recall_score(y_test, pred)\n",
    "f1 = f1_score(y_test, pred)\n",
    "print(\"precison:\", precision)\n",
    "print(\"recall:\", recall)\n",
    "print(\"accuracy:\", accuracy)\n",
    "print(\"f1:\", f1)\n",
    "\n",
    "# val\n",
    "pred = clf.predict(x_val)\n",
    "sample = val_df\n",
    "sample[\"Obesity\"] = pred\n",
    "sample = sample.drop([\"clean_words\"], axis=1)\n",
    "sample.to_csv(\"sample_submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b2f317",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a2a0eb7",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c603b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion_matrix: \n",
      " [[21  0]\n",
      " [ 1 18]]\n",
      "precison: 1.0\n",
      "recall: 0.9473684210526315\n",
      "accuracy: 0.975\n",
      "f1: 0.972972972972973\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "# test: 計算confusion_matrix和相關驗證數值\n",
    "pred = gnb.predict(X_test)\n",
    "c = confusion_matrix(y_test, pred)\n",
    "print(\"confusion_matrix: \\n\", c)\n",
    "accuracy = accuracy_score(y_test, pred)\n",
    "precision = precision_score(y_test, pred)\n",
    "recall = recall_score(y_test, pred)\n",
    "f1 = f1_score(y_test, pred)\n",
    "print(\"precison:\", precision)\n",
    "print(\"recall:\", recall)\n",
    "print(\"accuracy:\", accuracy)\n",
    "print(\"f1:\", f1)\n",
    "\n",
    "\n",
    "# val\n",
    "pred = gnb.predict(x_val)\n",
    "sample = val_df\n",
    "sample[\"Obesity\"] = pred\n",
    "sample = sample.drop([\"clean_words\"], axis=1)\n",
    "sample.to_csv(\"sample_submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38b3829",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02f51828",
   "metadata": {},
   "source": [
    "# XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "821a4029",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:57:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "confusion_matrix: \n",
      " [[21  0]\n",
      " [ 0 19]]\n",
      "precison: 1.0\n",
      "recall: 1.0\n",
      "accuracy: 1.0\n",
      "f1: 1.0\n"
     ]
    }
   ],
   "source": [
    "from xgboost.sklearn import XGBClassifier\n",
    "xgb = XGBClassifier(\n",
    "                    n_estimators=200,    #樹的個數\n",
    "                    learning_rate= 0.3,  # 如同學習率\n",
    "                    max_depth=50         # 構建樹的深度，越大越容易過擬合 \n",
    "                    )\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "# test: 計算confusion_matrix和相關驗證數值\n",
    "pred = xgb.predict(X_test)\n",
    "c = confusion_matrix(y_test, pred)\n",
    "print(\"confusion_matrix: \\n\", c)\n",
    "accuracy = accuracy_score(y_test, pred)\n",
    "precision = precision_score(y_test, pred)\n",
    "recall = recall_score(y_test, pred)\n",
    "f1 = f1_score(y_test, pred)\n",
    "print(\"precison:\", precision)\n",
    "print(\"recall:\", recall)\n",
    "print(\"accuracy:\", accuracy)\n",
    "print(\"f1:\", f1)\n",
    "\n",
    "\n",
    "# val\n",
    "pred = xgb.predict(x_val)\n",
    "sample = val_df\n",
    "sample[\"Obesity\"] = pred\n",
    "sample = sample.drop([\"clean_words\"], axis=1)\n",
    "sample.to_csv(\"sample_submission.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2367f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696ce2a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c00ed2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0d443d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
