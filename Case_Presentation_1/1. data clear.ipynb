{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a4a4439",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import string\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk import word_tokenize, pos_tag, corpus\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5fdd26",
   "metadata": {},
   "source": [
    "# 讀資料貼標"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83b963a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train / test / validation data\n",
    "# target = \"Train_Textual\"\n",
    "target = \"Test_Intuitive\"\n",
    "\n",
    "# 訓練資料檔案名稱\n",
    "listdirs = os.listdir(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28262b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [(text,y), (text,y), .....]\n",
    "data = []\n",
    "for filename in listdirs:\n",
    "    y = filename.split(\"_\")[0]        # 結果\n",
    "    with open(\"{}/{}\".format(target, filename), \"r\") as f:\n",
    "        text = f.read()\n",
    "    data.append((text.lower(), y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6890a3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 移除標點符號&數字\n",
    "def remove_punctuation(line):\n",
    "    rule = re.compile(r\"[^a-zA-Z]\")\n",
    "    line = rule.sub(' ',line)\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc314981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 獲取單詞的詞性\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d19b436",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a564a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "edfbd24a",
   "metadata": {},
   "source": [
    "# 資料清整"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99a8b638",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp = []\n",
    "temp1 = []\n",
    "for d in data:\n",
    "    text = d[0]\n",
    "    y = d[1]\n",
    "    y = 1 if y == \"Y\" else 0    #\"Y\"=1 其他=0\n",
    "    \n",
    "    # 移除標點符號\n",
    "    text = remove_punctuation(text)\n",
    "\n",
    "    # 斷字\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    \n",
    "    # 詞性\n",
    "    words_tags = pos_tag(words)     # 獲取單詞詞性\n",
    "    \n",
    "    \n",
    "    # 去除停用字 且 字長度>2\n",
    "    nltk_stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "    wnl = WordNetLemmatizer()\n",
    "    clean_words = []\n",
    "    for word_tag in words_tags:\n",
    "        wordnet_pos = get_wordnet_pos(word_tag[1]) or wordnet.NOUN\n",
    "        origin_word = wnl.lemmatize(word_tag[0], pos=wordnet_pos)    # 詞性還原\n",
    "#         print(word_tag[0], origin_word)\n",
    "        \n",
    "        if origin_word not in nltk_stopwords and len(origin_word) > 2:\n",
    "            clean_words.append(origin_word)\n",
    "            \n",
    "    temp.append((y, clean_words))                         # W2V不能去除重複字，不然算相似詞會有問題\n",
    "    temp1.append((y, list(set(clean_words))))             # 計算權重時去除重複字\n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1ebfbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(temp, columns=[\"y\", \"clean_words\"]).to_pickle(\"clean_{}.pk\".format(target))\n",
    "pd.DataFrame(temp1, columns=[\"y\", \"clean_words\"]).to_pickle(\"clean_{}_VAL.pk\".format(target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "20b13959",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9fbbf698",
   "metadata": {},
   "source": [
    "# validation也要整理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "108a3825",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"Validation\"\n",
    "\n",
    "# 訓練資料檔案名稱\n",
    "listdirs = os.listdir(target)\n",
    "\n",
    "# [(text,y), (text,y), .....]\n",
    "data = []\n",
    "for filename in listdirs:\n",
    "    with open(\"{}/{}\".format(target, filename), \"r\") as f:\n",
    "        text = f.read()\n",
    "    data.append((filename, text.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e12c6ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = []\n",
    "temp1 = []\n",
    "for d in data:\n",
    "    filename = d[0]\n",
    "    text = d[1]\n",
    "    \n",
    "    # 移除標點符號\n",
    "    text = remove_punctuation(text)\n",
    "\n",
    "    # 斷字\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    \n",
    "    # 詞性\n",
    "    words_tags = pos_tag(words)     # 獲取單詞詞性\n",
    "    \n",
    "    \n",
    "    # 去除停用字 且 字長度>2\n",
    "    nltk_stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "    wnl = WordNetLemmatizer()\n",
    "    clean_words = []\n",
    "    for word_tag in words_tags:\n",
    "        wordnet_pos = get_wordnet_pos(word_tag[1]) or wordnet.NOUN\n",
    "        origin_word = wnl.lemmatize(word_tag[0], pos=wordnet_pos)    # 詞性還原\n",
    "#         print(word_tag[0], origin_word)\n",
    "        \n",
    "        if origin_word not in nltk_stopwords and len(origin_word) > 2:\n",
    "            clean_words.append(origin_word)\n",
    "            \n",
    "    temp.append((filename, clean_words))                    # W2V不能去除重複字，不然算相似詞會有問題，因為還未有預測結果我先填None\n",
    "    temp1.append((filename, list(set(clean_words))))        # 計算權重時去除重複字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "13b998eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(temp, columns=[\"Filename\", \"clean_words\"]).to_pickle(\"clean_{}.pk\".format(target))\n",
    "pd.DataFrame(temp1, columns=[\"Filename\", \"clean_words\"]).to_pickle(\"clean_{}_VAL.pk\".format(target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad7d06c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
